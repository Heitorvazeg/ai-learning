# ü§ñ Redes Neurais (Neural Networks)

Redes neurais s√£o **modelos de aprendizado de m√°quina** inspirados no **c√©rebro humano**,
 compostos por **neur√¥nios artificiais** que aprendem a reconhecer padr√µes a partir de dados.

---

## üß† Estrutura B√°sica

- **Camada de entrada (Input Layer)**  
  Recebe os dados brutos de entrada.

- **Camadas ocultas (Hidden Layers)**  
  Realizam c√°lculos com **pesos** e aplicam **fun√ß√µes de ativa√ß√£o** para extrair padr√µes.

- **Camada de sa√≠da (Output Layer)**  
  Fornece a **resposta final** do modelo.

---

## üîπ Neur√¥nio

- Cada **neur√¥nio** recebe um valor entre `0` e `1`, chamado de **Activation**.
- A ativa√ß√£o representa **o qu√£o "ativo" (positivo)** aquele neur√¥nio est√° em rela√ß√£o ao padr√£o que ele reconhece.

### Elementos-chave:
- **Peso (Weight):** Intensidade da conex√£o entre neur√¥nios.
- **Bias (Vi√©s):** Valor adicional que define o qu√£o alto o sinal precisa ser para "fazer sentido".
- **Fun√ß√£o de ativa√ß√£o:** Garante que os valores fiquem dentro de um intervalo (ex: `sigmoid`, que for√ßa os valores entre 0 e 1).
Sem fun√ß√£o de ativa√ß√£o, redes neurais se tornam lineares; apenas somam valores.

---

## üéØ Como a rede aprende?

O processo de **aprendizado (learning)** √© encontrar a **quantidade certa de ativa√ß√£o** entre os neur√¥nios.
 Isso √© feito atrav√©s de algoritmos como:

- **Propaga√ß√£o para frente (forward propagation)**:  
  Os dados passam camada por camada at√© a sa√≠da.

- **C√°lculo do erro**:  
  A sa√≠da gerada √© comparada com a sa√≠da correta (esperada).

- **Retropropaga√ß√£o (backpropagation)**:  
  O erro √© distribu√≠do para os neur√¥nios anteriores e os **pesos s√£o ajustados** com base nesse erro.
  Descer o Loss que √© uma fun√ß√£o quadr√°tica.
  w0.new = w0.old + a* -> calculo de pesos apos pegar a loss.

  o "custo" pega os valores finais de cada neor√¥nio e faz: (valor - esperado)^2
  Soma todos os custos. Quando maior a dist√¢ncia do valor ao valor esperado maior ser√° o
  custo e assim maior ser√° a mudan√ßa dos pesos. Quanto menor, menor.

  Aprender √© apenas diminuir o custo.

  Gradient of the cost -> Quanto cada n√≥ deve mudar para chegar ao resultado.

---

## üß© Exemplo de Aplica√ß√£o

Imagine uma rede neural que reconhece **n√∫meros manuscritos** com base em **pixels** de uma imagem:

- Os **neur√¥nios da entrada** recebem os pixels.
- As **camadas ocultas** detectam padr√µes como linhas, curvas, cantos.
- A **√∫ltima camada** determina **qual n√∫mero est√° na tela**.

Para refor√ßar o aprendizado:
- A rede aplica **pesos positivos** para padr√µes desejados.
- Aplica **pesos negativos** ao que est√° fora do padr√£o.
- A **fun√ß√£o sigmoid** garante que as ativa√ß√µes fiquem entre 0 e 1.

---

## TensorFlow:
- Open Source library que ajuda a implementar algoritmos ML.
- Utilizado muito para redes neurais.

---