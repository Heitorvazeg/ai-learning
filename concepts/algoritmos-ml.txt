# Modelos de Machine Learning

## k-Nearest Neighbors (KNN)

- Utiliza um plano cartesiano para analisar dados.
- Cada ponto representa alguma caracterÃ­stica, e sua posiÃ§Ã£o no plano reflete detalhes que podem causar determinada caracterÃ­stica.
- Os vizinhos mais prÃ³ximos sÃ£o considerados porque pontos prÃ³ximos no plano tendem a ser parecidos.
- A distÃ¢ncia entre pontos Ã© importante para determinar a proximidade.

### DistÃ¢ncia Euclidiana
- Ã‰ a distÃ¢ncia entre dois pontos no espaÃ§o.
- FÃ³rmula da distÃ¢ncia entre pontos:

d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}

- O valor de **K** representa a quantidade de vizinhos usados na anÃ¡lise.
- Exemplo: Se \( K = 3 \), a anÃ¡lise considera os 3 pontos mais prÃ³ximos.
- Pode ser extrapolado para espaÃ§os de mÃºltiplas dimensÃµes (ex: tridimensional).

---

## Naive Bayes

- Baseado na **Regra de Bayes**.
- Ã‰ uma suposiÃ§Ã£o ingÃªnua sobre a probabilidade dos eventos.
- Regra de Bayes:


P(A|B) = {P(B|A) * P(A)}/{P(B)}


- \( P(A|B) \): probabilidade de \( A \) acontecer dado que \( B \) ocorreu.
- Para cada classe, calcula a probabilidade de os dados analisados pertencerem a ela e escolhe a classe com maior probabilidade.
- SupÃµe que todas as features sÃ£o independentes entre si (o que nem sempre Ã© verdade).

---

## ğŸ“ˆ Logistic Regression

A **RegressÃ£o LogÃ­stica** Ã© um algoritmo usado para **prever a probabilidade de ocorrÃªncia de um evento binÃ¡rio**,
 ou seja, algo que tem apenas duas possÃ­veis saÃ­das (ex: 0 ou 1, verdadeiro ou falso, positivo ou negativo).

### âœ³ï¸ CaracterÃ­sticas

- Algoritmo **supervisionado**.
- Utiliza **variÃ¡veis independentes** para prever a probabilidade de um resultado binÃ¡rio.
- Aplica a **funÃ§Ã£o sigmoide** para mapear os valores de entrada em uma **probabilidade entre 0 e 1**.

### ğŸ“Œ FÃ³rmula da Sigmoid

```math
S(y) = \frac{1}{1 + e^{-y}}

# ğŸ“Š Support Vector Machines (SVM)

**Support Vector Machines (SVM)** sÃ£o algoritmos de aprendizado de mÃ¡quina **supervisionados** usados principalmente para **classificaÃ§Ã£o** e, em alguns casos, para **regressÃ£o**.

---

## ğŸ§  IntuiÃ§Ã£o

O objetivo do SVM Ã© encontrar **uma linha (em 2D)** ou **hiperplano (em n dimensÃµes)** que **melhor separa duas classes**.

- Essa linha deve dividir os dados de forma que cada classe fique de um lado.
- O SVM **procura o â€œmeio perfeitoâ€** entre as classes.
- Se a linha estiver mais prÃ³xima de uma das classes, a previsÃ£o pode piorar.

---

## ğŸ“ Margem

A **margem** Ã© a distÃ¢ncia entre o hiperplano (linha de separaÃ§Ã£o) e os pontos de cada classe mais prÃ³ximos a ele (chamados de **vetores de suporte**).

- O SVM **tenta maximizar essa margem**.
- Uma **margem maior** geralmente significa uma **melhor generalizaÃ§Ã£o**.

---

## ğŸ¯ Kernel Trick

O **Kernel Trick** Ã© uma tÃ©cnica que permite ao SVM lidar com dados **nÃ£o linearmente separÃ¡veis**, ou seja, dados que nÃ£o podem ser separados por uma linha reta no espaÃ§o original.
